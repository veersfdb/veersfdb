What are the four ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—² ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ ğ—§ğ˜†ğ—½ğ—²ğ˜€?
 
Even if you will not work with them day to day,  the following are the four ways to deploy a ML Model you should know and understand as a MLOps/ML Engineer.
 
â¡ï¸ ğ—•ğ—®ğ˜ğ—°ğ—µ: 
 
ğŸ‘‰ You apply your trained models as a part of ETL/ELT Process on a given schedule.
ğŸ‘‰ You load the required Features from a batch storage, apply inference and save the results to a batch storage.
ğŸ‘‰ It is sometimes falsely thought that you canâ€™t use this method for Real Time Predictions.
ğŸ‘‰ Inference results can be loaded into a real time storage and used for real time applications.
 
â¡ï¸ ğ—˜ğ—ºğ—¯ğ—²ğ—±ğ—±ğ—²ğ—± ğ—¶ğ—» ğ—® ğ—¦ğ˜ğ—¿ğ—²ğ—®ğ—º ğ—”ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»: 
 
ğŸ‘‰ You apply your trained models as a part of Stream Processing Pipeline.
ğŸ‘‰ While Data is continuously piped through your Streaming Data Pipelines, an application with a loaded model continuously applies inference on the data and returns it to the system - most likely another Streaming Storage.
ğŸ‘‰ This deployment type is likely to involve a real time Feature Store Serving API to retrieve additional Static Features for inference purposes.
ğŸ‘‰ Predictions can be consumed by multiple applications subscribing to the Inference Stream.
 
â¡ï¸ ğ—¥ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ - ğ—¥ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—²:
 
ğŸ‘‰ You expose your model as a Backend Service (REST or gRPC).
ğŸ‘‰ This ML Service retrieves Features needed for inference from a Real Time Feature Store Serving API.
ğŸ‘‰ Inference can be requested by any application in real time as long as it is able to form a correct request that conforms API Contract.
 
â¡ï¸ ğ—˜ğ—±ğ—´ğ—²: 
 
ğŸ‘‰ You embed your trained model directly into the application that runs on a user device.
ğŸ‘‰ This method provides the lowest latency and improves privacy.
ğŸ‘‰ Data in most cases is generated and lives inside of device significantly improving the security.
 
What types of deployments are you mostly working on? Let me know in the comments!  ğŸ‘‡

--------